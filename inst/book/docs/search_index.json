[["data-in-the-nigeriapitaudit-package.html", "Chapter 2 Data in the nigeriaPITaudit Package", " Chapter 2 Data in the nigeriaPITaudit Package The nigeriaPITaudit package is a toolkit designed to support the auditing of Pay-As-You-Earn (PAYE) tax systems in Nigeria. It includes built-in datasets like fmc2017, which serve as examples for testing, analysis, and training. These datasets allow users—especially tax analysts and auditors—to explore PAYE data patterns, evaluate compliance, and apply audit techniques using R. This chapter introduces the purpose, structure, and usage of datasets included in the package. 1.1 Introduction to Datasets in R Packages Datasets play a pivotal role in R packages, especially for those focused on data analysis, statistical modeling, or providing educational resources. Including datasets in an R package is essential for demonstrating the functionality of the package and offering examples that users can easily replicate. This ensures that the package is accessible and usable even without needing external data sources. For example, in the nigeriaPITaudit package, the dataset fmc2017 is included to provide users with real-world data that can be used directly in examples, functions, or analyses without the need to download or recreate the data. This improves the usability and reproducibility of the package, making it easier for users to apply the package to their own work. 1.2 What is a Dataset in an R Package? In the simplest terms, a dataset is a collection of data organized in a structured format. In R, datasets are often stored as data frames, which are tables where each column represents a variable and each row represents an observation. Datasets in R can also exist as lists, vectors, or other data structures depending on the data type and how it is intended to be used. For example, in the nigeriaPITaudit package, the dataset fmc2017 is stored as a data frame, which consists of multiple columns, each representing different variables related to the data from the year 2017 in the Nigerian audit. Common dataset structures include: • Data Frames: The most common type, where each column represents a variable and each row represents an observation. • Lists: Used when the dataset consists of different types of data that may not fit neatly into a table. • Vectors: Used for a single variable or a one-dimensional set of data. 1.3 Why Include Datasets in R Packages? Including datasets in R packages provides several benefits to both the developers and users: • For Learners and Analysts: Datasets are critical for learning purposes and provide a hands-on way to explore the functionality of a package. For instance, with the fmc2017 dataset, users can apply various analysis functions included in the package to the dataset and gain immediate insights into how the package operates. • Avoiding the Need to Recreate or Download Data: Including datasets directly in the package saves users time, as they don’t need to search for or download the data separately. This is especially useful for beginner-level users or those without easy access to the data. • Improving Reproducibility and Consistency: When datasets are bundled with an R package, users can be confident that they are working with the same data as the examples or analyses in the package documentation. This ensures consistent results and improves the reproducibility of research. By including datasets in a package, you can streamline the learning experience and enhance the utility of the package for a broader range of users. 1.4 Exploring Datasets in nigeriaPITaudit The nigeriaPITaudit package includes several datasets, with fmc2017 being one of the primary datasets. This dataset contains data related to the Nigerian financial and fiscal management practices from the year 2017. By incorporating this data into the package, users can explore real-world data relevant to audits and financial analysis in the context of Nigeria. Datasets in the package are typically stored in .rda format, a compressed and efficient R data format that allows for quick loading and saving of large datasets. The datasets are located in the data/ folder of the package structure. To give you an overview, here are the steps for loading and working with the fmc2017 dataset: File Format: .rda (a compressed R data format) • Location: The dataset is stored in the data/ folder of the package directory. Example usage: To access the dataset, use the data() function library(nigeriaPITaudit) data(fmc2017) After loading the dataset, you can explore it using R functions like str(), summary(), and head() to understand its structure, contents, and statistical summaries. 1.5 Accessing and Loading Package Datasets Accessing and loading datasets from an R package is straightforward. The data() function is used to load a dataset into the R environment. Here’s how to use it with the fmc2017 dataset: Listing Datasets: To list all available datasets in the nigeriaPITaudit package, you can use the following command: data(package = &quot;nigeriaPITaudit&quot;) Loading a Dataset: To load the fmc2017 dataset, simply use: data(fmc2017) Exploring the Dataset: Once the dataset is loaded, you can use functions like str(), summary(), and head() to get more information about its structure and contents. o str(): Provides the structure of the dataset, showing data types and variable names. o summary(): Generates summary statistics (e.g., mean, min, max) for each variable in the dataset. o head(): Displays the first few rows of the dataset to give you a quick preview. Example: str(fmc2017) # Check the structure of the dataset summary(fmc2017) # Get summary statistics head(fmc2017) # Preview the first few rows of the dataset 1.6 When to Include a Dataset in a Package There are several scenarios where it makes sense to include a dataset in an R package: • Datasets Used in Vignettes or Examples: If your package includes vignettes (tutorials or detailed examples), datasets can be essential for demonstrating package functionality. The fmc2017 dataset could be used to showcase different audit-related functions in the package. • Datasets as Core Input for Package Functions: If your package’s core functions rely on a specific dataset for operation, such as financial audits, it’s crucial to bundle that dataset with the package. • Synthetic or Anonymized Data: In some cases, especially for privacy or security reasons, you might want to include anonymized or synthetic datasets. This ensures users can still test the package without violating privacy. 1.7 Summary In this chapter, we discussed the role of datasets in R packages and why it is beneficial to include them. Datasets enhance reproducibility, learning, and the overall functionality of a package. The nigeriaPITaudit package includes valuable datasets, such as fmc2017, which can be loaded and explored using simple R functions. By including these datasets, users can easily follow along with examples and tutorials, making the package more practical and accessible. The next chapter will dive into the tools and methods used to manage datasets within an R package, including documentation, testing, and preprocessing steps. "],["tools-for-managing-datasets-for-an-r-package.html", "Chapter 3 Tools for Managing Datasets for an R Package", " Chapter 3 Tools for Managing Datasets for an R Package Managing datasets within an R package is an essential part of the package development process. Datasets must be prepared, documented, tested, and distributed efficiently, and there are a range of tools that help accomplish these tasks seamlessly. In this chapter, we will explore the key tools commonly used in R package development, such as devtools, usethis, roxygen2, and testthat. These tools help streamline the process of dataset integration, documentation, testing, and overall package development. We will delve into the functionality of these tools and show how they can be used to manage your datasets effectively. 2.1 Introduction to Dataset Management Tools In R package development, effective management of datasets involves more than just loading and saving data. It requires the integration of datasets into the package structure, documenting their usage, ensuring data integrity, and performing systematic testing. Tools are essential for facilitating each of these tasks, automating common processes, and improving the overall efficiency of package development. Without the right tools, managing datasets can become cumbersome, especially when working with large datasets or complex workflows. By using the correct tools, package developers can: • Streamline the integration of datasets: Tools like devtools and usethis simplify the process of adding, saving, and building datasets into the package. • Document datasets comprehensively: roxygen2 allows for seamless documentation of datasets within the package, ensuring that users can understand and make use of the data. • Test dataset integrity: testthat facilitates the creation of tests for validating dataset contents, such as checking for missing values, proper column names, or data types. • Distribute datasets efficiently: Proper packaging of datasets using tools like tools ensures that they are appropriately compressed and formatted for distribution to users. In the following sections, we will explore these tools in greater detail and show how to use them for managing datasets effectively within an R package. 2.2 Overview of Core Tools for Dataset Management There are several core tools in the R ecosystem that help in managing datasets for R packages. Let’s look at these tools, their functionalities, and how they fit into the package development workflow. 2.2.1 devtools • devtools is a comprehensive tool that simplifies many aspects of R package development. It includes functions for creating packages, building, testing, and loading the package. It also helps with managing datasets, specifically by using the use_data() function to save datasets in a structured and reproducible way. 2.2.2 usethis • usethis is designed to make package development easier by creating common files, folders, and structures. It allows you to create a data-raw/ folder for raw data, automate the generation of functions for data preprocessing, and streamline the management of package dependencies. 2.2.3 roxygen2 • roxygen2 is used for documenting R functions and datasets. It enables you to generate documentation directly from R code using special comments (tags like @format, @source, etc.). This tool is integral to ensuring that the datasets within the package are well-documented and easily accessible to users. 2.2.4 testthat • testthat is a testing framework for R that allows you to write unit tests for your package. With testthat, you can create tests to ensure that datasets are consistent and meet specified conditions (e.g., checking if variables have the correct types, whether there are missing values, etc.). 2.2.5 Data Preprocessing Tools (Optional) • data.table: A package optimized for high-performance data manipulation, useful when preprocessing large datasets before they are saved into your package. • dplyr: A package that provides a set of tools for data manipulation, such as filtering, grouping, and summarizing data. • readr: A package that helps with reading and writing data files efficiently, including CSV and other text-based formats. By leveraging these tools, you can effectively manage the datasets included in your R package and ensure that they are clean, well-documented, and tested. 2.3 Using the devtools Package devtools is a primary tool for package development in R. It simplifies many tasks related to package creation, building, and testing. Below are the key functions in devtools that you’ll use when managing datasets. 2.3.1 Creating Packages: • The devtools::create() function allows you to create a new R package. It sets up the necessary structure for your package, including folders for R functions, data, and documentation. • Example: # devtools::create(&quot;myPackage&quot;) 2.3.2 Loading and Building Packages: • devtools::load_all() loads the entire package so that any changes made are reflected immediately, allowing you to test your code without rebuilding the package every time. • Example: # devtools::load_all() 2.3.3 Working with use_data(), check(), and load_all(): • use_data(): Saves your dataset in the data/ directory of your package. It also ensures the dataset is saved in an efficient .rda format. # devtools::use_data(my_dataset) • check(): Runs various checks on your package, ensuring it follows R’s package standards and is ready for submission to CRAN. # devtools::check() 2.4 Using the usethis Package usethis is a powerful tool that automates common tasks in package development, such as creating folders and files, managing dependencies, and simplifying the process of creating R functions for reproducible workflows. 2.4.1 Creating Folders Like data-raw/: • The usethis::use_data_raw() function creates the data-raw/ folder, which is used for storing raw datasets that need preprocessing before they are included in the package. # usethis::use_data_raw() 2.4.2 Using use_data_raw() and use_data(): • use_data_raw(): This function helps you set up the infrastructure for raw data. You can then preprocess and save the data using use_data() once it’s ready. • use_data(): Saves preprocessed datasets into the data/ folder in .rda format. 2.4.3 Creating .R Files for Reproducible Preprocessing:, • It is essential to ensure that any preprocessing steps applied to raw data are reproducible. usethis can generate .R files containing the code to preprocess data, ensuring others can reproduce the steps. 2.5 Using the roxygen2 Package roxygen2 is an essential tool for documenting your datasets and functions within an R package. It allows you to annotate your R code with special comments, which are then converted into formal documentation. 2.5.1 Documenting Datasets with @format, @source, @examples: • @format: Describes the structure of the dataset (e.g., the number of rows, columns, and the types of variables). • @source: Provides a reference to the source of the dataset, such as the original publisher or data repository. • @examples: Includes usage examples, showing how to load and use the dataset in R. Example documentation: #&#39; My Dataset #&#39; #&#39; A dataset containing information about XYZ. #&#39; #&#39; @format A data frame with 100 rows and 5 columns: #&#39; \\describe{ #&#39; \\item{column1}{Description of column 1} #&#39; \\item{column2}{Description of column 2} #&#39; } #&#39; @source Source of the dataset #&#39; @examples #&#39; data(my_dataset) #&#39; head(my_dataset) &quot;my_dataset&quot; 2.5.2 Running devtools::document(): • After annotating your dataset with roxygen2 comments, use devtools::document() to generate the documentation files for your package. # devtools::document() 2.6 Using the testthat Package testthat allows you to write unit tests for your datasets to ensure data integrity and consistency. These tests can verify that the dataset meets specified conditions, such as having the correct column names, types, and handling missing values. 2.6.1 Writing Tests for Your Datasets: • Example tests include verifying column names, ensuring no missing values, or checking for specific data types in columns. test_that(“dataset has expected columns”,{ expect_true(“column1” %in% colnames(my_dataset)) }) 2.6.2 Integrating with Continuous Testing Workflow: • Integrate testthat into your continuous integration (CI) system to automatically test your datasets whenever changes are made. 2.7 Other Helpful Tools and Utilities In addition to the core tools mentioned above, there are several other utilities that can assist with managing datasets. 2.7.1 tools::checkRdaFiles() and tools::resaveRdaFiles(): resaveRdaFiles() is designed to work with R data files (.Rda or .rda), not Excel files. Solution If you want to work with the data in R, you’ll need to: Read the Excel file into R using a package like readxl or openxlsx. Save the data as an R data file (.Rda or .rda) using save(). Here’s an example: Load the readxl package library(readxl) Read the Excel file data &lt;- read_excel(“C:/Users/Lawrence Daniel/Downloads/fmc20172.xlsx”) Save the data as an R data file save(data, file = “fmc20172.Rda”) After saving the data as an R data file, you can use resaveRdaFiles() if needed. However, it’s likely that you won’t need to use resaveRdaFiles() unless you’re working with package development. When developing a package, it’s common to store data in the data directory. Here’s the procedure: Step 1: Prepare the Data 1. Load your data into R (e.g., using read_excel()). 2. Clean, transform, and prepare the data as needed. Step 2: Save the Data as an R Data File 1. Use save() to save the data as an R data file (.Rda or .rda) in the data directory of your package. 2. Make sure to use compress = TRUE to reduce file size. Example: save(data, file = “path/to/your/package/data/fmc20172.Rda”, compress = TRUE) Step 3: Document the Data 1. Create a documentation file for the data in the R directory (e.g., fmc20172.R). 2. Use roxygen2 tags to document the data. Example: #&#39; FMC 2017 Data #&#39; #&#39; Description of the data #&#39; #&#39; @format A data frame with columns ... &quot;fmc20172&quot; Step 4: Load the Data in the Package 1. When you build and install the package, the data will be available for use. 2. You can load the data using data(“fmc20172”, package = “yourPackageName”). Using tools::resaveRdaFiles() If you have multiple data files in your data directory, you can use ‘tools::resaveRdaFiles()’ to update them. However, this is typically not necessary unless you’re making changes to the data or package structure. In our case, using ‘save(data, file = “fmc20172.Rda”)’ directly into the target folder (data directory) is a good approach. Just make sure to follow the steps above to document the data properly. When using ’tools::resaveRdaFiles()', you typically need to specify the path to the data directory. Syntax The syntax for ’resaveRdaFiles(); is: tools::resaveRdaFiles(&quot;path/to/your/package/data&quot;) Purpose By specifying the path to the data directory, ‘resaveRdaFiles()’ will update all R data files (.Rda or .rda) in that directory. Example If your package directory is mypackage, and the data directory is inside it, you would use: tools::resaveRdaFiles(&quot;path/to/mypackage/data&quot;) Note Make sure to run ‘resaveRdaFiles()’ from the parent directory of your package or specify the full path to the data directory. By using ‘resaveRdaFiles()’ with the correct path, you can efficiently update all R data files in your package’s data directory. Here’s an example of how to use ‘tools::resaveRdaFiles()’ with your directory path: tools::resaveRdaFiles(&quot;C:/Users/Lawrence Daniel/Desktop/nigeriaPITaudit/data&quot;) Note that w’ve used forward slashes (/) instead of backslashes () in the path. Both should work, but forward slashes are often more convenient in R. By running this command, ‘resaveRdaFiles()’ will update all R data files (.Rda or .rda) in the data directory inside your nigeriaPITaudit package directory. This will update all R data files in the data directory, including fmc2017.Rda if it exists. Note Make sure that the fmc2017.Rda file is in the correct format (i.e., an R data file saved using save()) and is located in the data directory. ‘checkRdaFiles()’: Checks if .rda files are properly formatted and can be loaded correctly. # tools::checkRdaFiles(&quot;path/to/dataset.rda&quot;) • resaveRdaFiles(): Resaves .rda files with compression to optimize file size. • tools::resaveRdaFiles(“path/to/dataset.rda”) 2.7.2 Other Helpful Packages • janitor: This package provides functions for cleaning data, such as removing empty columns or rows, renaming variables, and checking for duplicate values. • readr: A package for reading and writing data files efficiently, especially useful for importing data before it is processed and saved into your package. • data.table: A high-performance package for manipulating large datasets efficiently. 2.8 Conclusion The tools discussed in this chapter are vital for effective dataset management in R package development. By leveraging tools like devtools, usethis, roxygen2, and testthat, you can streamline the process of adding, documenting, testing, and distributing datasets within your R package. These tools not only improve the efficiency of your workflow but also help maintain high standards for data integrity and reproducibility. As your package evolves, these tools will ensure that your datasets are well-managed and accessible to other users. "],["preparing-your-datasets-for-an-r-package.html", "Chapter 4 Preparing Your Datasets for an R Package", " Chapter 4 Preparing Your Datasets for an R Package Preparing datasets for inclusion in an R package is a critical process. In this chapter, we will walk through the key steps involved in preparing your datasets, from initial selection to choosing the right format. Each section will detail best practices, tools, and methods for ensuring your datasets are ready for efficient use and integration. 3.1 Selecting a Suitable Dataset Selecting an appropriate dataset is the first and most important step in preparing data for an R package. A good dataset should meet several criteria, ensuring that it adds value to your package without overwhelming users or introducing complications. 1. What Makes a Dataset Suitable for Packaging? • Ethical Considerations: Ensure that the dataset complies with licensing laws and does not include personally identifiable information (PII) unless it has been anonymized. It should also adhere to any legal or ethical restrictions related to its usage. • Licensing: The dataset should be released under a license that allows for redistribution, modification, and use. Be clear about the dataset’s license in your package documentation. • Relevance and Completeness: The dataset should be relevant to your package’s functionality, providing valuable insights or data for users. It must also be comprehensive enough to support real-world applications. • Size: Ideally, the dataset should not be too large. Large datasets can cause issues with memory usage, loading times, and the overall performance of the package. Keep it as small as possible while still being useful. • Examples of Good vs. Bad Candidates for Packaging: o Good: A small, cleaned, well-documented dataset of economic indicators for a financial analysis package. o Bad: A large dataset containing raw, messy data that requires significant cleaning and is difficult to manage. 3.2 Exploring the Dataset Before cleaning and preprocessing, it is essential to explore the dataset in detail. This helps to understand its structure, variables, and potential issues. 1. Initial Exploration: Structure, Variables, Data Types • Structure: Use functions like str() to inspect the structure of your dataset. This will provide information on the number of rows, columns, and the type of each variable. • Variables and Data Types: Ensure you know what each variable represents and its appropriate type (e.g., numeric, factor, or character). 2. Summary Statistics and Visual Inspection • Summary Statistics: Functions like summary() provide an overview of the distribution of each variable, including central tendencies and missing values. • Visual Inspection: Use visual tools (e.g., ggplot2 or plotly) to detect patterns, outliers, or any data inconsistencies. 3. Tools for Dataset Exploration: • skimr::skim(): This function provides a summary of all columns with extended statistics, including min, max, and NA counts. • glimpse(): A function from the dplyr package, giving a quick look at the dataset’s structure and data types. 3.3 Cleaning and Preprocessing the Data Once you understand the dataset, the next step is cleaning and preprocessing it. Cleaning data ensures that it is consistent, usable, and ready for analysis or inclusion in your package. 1. Handling Missing Values • Remove Missing Values: Use na.omit() or drop_na() (from tidyverse) to eliminate rows with missing values. • Impute Missing Values: In some cases, imputing missing values may be necessary. This can be done by replacing missing values with the mean, median, or a prediction model. 2. Renaming Variables • Clear and consistent variable names are crucial. Use dplyr::rename() to give variables meaningful names that align with the purpose of the dataset. 3. Recoding Factors • If your dataset includes categorical variables, use factor() to ensure they are appropriately coded. • Recoding might also involve converting levels of factors, or transforming variables into more meaningful groups using dplyr::mutate(). 4. Normalizing Formats • Ensure consistency across the dataset. For example, make sure all dates are in the same format, all text is lowercase, and numeric variables are in a standardized format. Example Cleaning Pipelines: • Using dplyr: cleaned_data &lt;- raw_data %&gt;% mutate(age = ifelse(age &lt; 18, NA, age)) %&gt;% filter(!is.na(age)) %&gt;% rename(income = salary) • Using data.table: library(data.table) raw_data &lt;- as.data.table(raw_data) raw_data[age &lt; 18, age := NA] raw_data &lt;- raw_data[!is.na(age)] setnames(raw_data, &quot;salary&quot;, &quot;income&quot;) 3.4 Choosing the Right Data Format After cleaning, the next consideration is the format in which you will save your dataset. The right format depends on factors like file size, loading times, and the types of operations you’ll be performing. 1. Comparison: .csv vs .rds vs .RData vs .feather • CSV: Human-readable, but inefficient for large datasets. It also doesn’t preserve R-specific attributes. • RDS: Preserves the full structure of R objects, including factors and dates. Ideal for saving single objects. • RData: Suitable for saving multiple R objects, but less efficient when working with single datasets. • Feather: A fast binary format that is compatible with both R and Python, great for sharing data across languages. 2. Tibble vs. Data Frame vs. Data Table: • Tibble: A modern alternative to data.frame. It does not convert strings to factors by default, which is useful for data integrity. • Data Frame: The standard R object for storing tabular data. • Data Table: An extension of data.frame optimized for large datasets, with improved performance in data manipulation. 3. Recommended Format for Package Datasets: .rda • The most efficient and preferred format for saving datasets in an R package is .rda. Use devtools::use_data() to save datasets in this format, which is both compressed and fast to load. 3.5 Managing Dataset Size and Compression Large datasets can increase the size of your R package and hinder performance. Here are some techniques to efficiently manage dataset size and ensure optimal performance: 1. Compressing Datasets • Use the compress argument in devtools::use_data() to save datasets in compressed format. The xz compression method is highly efficient. 2. Stripping Environments • Ensure that you strip unnecessary environments from your R objects before saving them. Use tools::resaveRdaFiles() to remove extra data and reduce file size. 3. Tools for Checking and Compressing R Objects • tools::checkRdaFiles(): Validates RData files and ensures they can be loaded without errors. • tools::resaveRdaFiles(): Resaves .rda files with compression to minimize file sizes. 4. Converting from CSV to .rda Efficiently • Convert large CSV files to .rda format using R’s read.csv() or readr::read_csv(), then save them in .rda format using devtools::use_data(). "],["adding-datasets-to-an-r-package.html", "Chapter 5 Adding Datasets to an R Package", " Chapter 5 Adding Datasets to an R Package In R package development, multiple ways exist to store and access data efficiently, each serving a specific purpose while balancing ease of use, memory efficiency, and reproducibility. R packages offer different storage locations for datasets, ensuring usability, efficiency, and security. Some datasets are publicly accessible, making them easy to load and use, while others are stored in restricted locations to maintain integrity and optimise package performance. 4.1 Creating Directories Before adding datasets to an R package, the appropriate directories must be created. These directories help organize raw, processed, and related scripts within the package structure. 4.1.1 Using usethis::use_data() The simplest way to create the data/ directory in an R package is by using the usethis package. The following function automatically generates the data/ directory if it does not exist: usethis::use_data() This function is particularly useful when working within an R package, ensuring that the data/ directory follows best practices for package development. 4.1.2 Creating a Directory Manually Alternatively, you can create a directory manually before saving the dataset using the dir.create() function. This allows for more control over the structure and ensures that the directory exists before attempting to store data. dir.create(&quot;data&quot;, showWarnings = FALSE) The argument showWarnings = FALSE is used to suppress any warning messages in case the directory already exists. This prevents unnecessary warnings while ensuring that the directory is created only when needed. 4.1.3 Using fs::dir_create() (from the fs package) The fs package provides a more modern and reliable way to create directories: fs::dir_create(&quot;data&quot;) This method is useful because it: • Creates parent directories if they don’t exist. • Works consistently across different operating systems. • Returns a more structured output. 4.1.4 Using Base R with if (!dir.exists()) Instead of just dir.create(), you can first check if the directory exists before creating it: if (!dir.exists(&quot;data&quot;)) { dir.create(&quot;data&quot;) } This ensures that the function only creates the directory if it doesn’t already exist, preventing unnecessary warnings. By using these methods, you can properly set up directories to store and manage datasets within your R package. In the next sections, we will explore how to add datasets and choose the appropriate storage formats. 4.2 Methods of Adding Datasets to an R Package This section discusses different methods for adding data to an R package, specifically focusing on how to move files from local storage or download them from online sources into the appropriate directory. The method for adding data to an R package depends on its source. If the dataset is local (already on your computer), it can be directly included in the package. On the other hand, if you need to download data from an online source, additional steps may be required to ensure proper integration. 4.2.1 Moving Local Files to data/ If you already have a dataset on your local computer (e.g., fmc2017.xlsx) that you want to add to your R package structure, first, you need to locate the existing data file and load the data into R: fmc2017 &lt;- readxl::read_excel(&quot;C:/Users/Lawrence Daniel/Downloads/fmc2017.xlsx&quot;, sheet = 1) This line of code reads an Excel file named fmc2017.xlsx from a specific location on my computer (in this case, the Downloads folder under the Users/username directory). The data from the first sheet of the Excel file is then loaded into R and assigned to a variable called fmc2017 for further use and manipulation. After the data is loaded into R, you can proceed to the next steps for saving it and integrating it into your R package structure, which we will cover in Section 4.2.2 Copying Files Directly into data/ Sometimes, you may want to copy your data into your target directory using the following code: file.copy(&quot;C:/Users/Lawrence Daniel/Downloads/infosightsConsulting.xlsx&quot;, &quot;data/infosightsConsulting.xlsx&quot;, overwrite = TRUE) The argument overwrite = TRUE is used when you want to update or replace an existing dataset in data/.. 4.2.3 Alternative Method: Using fs::file_copy() Alternatively, you can use the fs package to copy files, which may offer more robust and platform-independent functionality. Here’s how you would do it: fs::file_copy(&quot;C:/Users/Lawrence Daniel/Downloads/infosightsConsulting.csv&quot;, &quot;data/infosightsConsulting.csv&quot;, overwrite = TRUE) This method behaves similarly to file.copy() but is part of the fs package, which is known for handling file operations more efficiently across different operating systems. 4.2.2 Online Sources (download.file()) If the dataset is hosted online (e.g., from a website or repository), you can download it directly into your R package: download.file(&quot;https://example.com/dataset.xlsx&quot;, &quot;data/dataset.xlsx&quot;, mode = &quot;wb&quot;) Or: url &lt;- &quot;https://example.com/dataset.csv&quot; destfile &lt;- &quot;inst/extdata/dataset.csv&quot; download.file(url, destfile) For datasets requiring authentication or API requests, use the httr or any other R package: library(httr) url &lt;- &quot;https://api.example.com/data&quot; response &lt;- GET(url) data &lt;- content(response, &quot;parsed&quot;) # Convert JSON response to R object save(data, file = &quot;data/data.rda&quot;) # Save the processed data 4.3 Saving Datasets as R Objects (.rda, .RDS) After adding datasets to your R package, it is crucial to save them in the correct format so they can be easily accessed and used within the package. R provides two primary formats for saving datasets: .rda and .RDS. These formats are specifically used to store R objects (like data frames, lists, etc.) for later use, and each has its specific use cases. Before saving any dataset, ensure that the data has been loaded into R as discussed in Section 4.2.1. In our example, we loaded the fmc2017 dataset from a local file into R using the following code: fmc2017 &lt;- readxl::read_excel(&quot;C:/Users/Lawrence Daniel/Downloads/fmc2017.xlsx&quot;, sheet = 1) Once the dataset is loaded, you can proceed to save it in the appropriate format. Let’s explore the methods to save datasets effectively in .rda and .RDS formats. 4.3.1 Using usethis::use_data() for Saving Datasets One of the most common methods to save datasets in an R package is by using the usethis::use_data() function. This function is designed specifically to save R objects in the data/ directory of your R package in .rda format. Here’s an example of how you can use it: usethis::use_data(fmc2017, overwrite = TRUE) This line of code saves the fmc2017 dataset into the data/ directory of your R package as an .rda file. The overwrite = TRUE argument ensures that any existing dataset with the same name is replaced. 4.3.2 Features of .rda files: • .rda files are often used for storing multiple R objects in one file. You can store several datasets together, making it convenient for packaging them. • It is a binary format that R uses to save objects efficiently. • These files are typically used for datasets that are part of an R package and are loaded automatically with the package. 4.3.3 Saving Multiple R Objects Using .rda You can save multiple R objects in a single .rda file. Let’s say we have two datasets, fmc2017 and infosightsConsulting, and we want to save them together in a single .rda file. Here’s how you can do that: save(fmc2017, infosightsConsulting, file = &quot;data/datasets.rda&quot;) This line of code saves both the fmc2017 and infosightsConsulting datasets into a single .rda file called datasets.rda in the data/ directory. You can then load this file later and access both datasets in one go. 4.3.4 Features of .rda files for Multiple Objects: • .rda allows you to bundle multiple R objects (datasets, lists, etc.) into a single file. • This is especially useful when you need to package several related datasets together, as it reduces the number of files in your project. 4.3.5 Saving Data Using save() for .rda Files Alternatively, you can save datasets using the save() function. This function is more general and can save one or more R objects into a binary .rda file. save(fmc2017, file = &quot;data/fmc2017.rda&quot;) This command will save the fmc2017 dataset to the data/ directory as an .rda file. The .rda format is preferred when you want to bundle multiple R objects into one file and use them within your package. 4.3.6 Features of save(): • You can save multiple objects in one .rda file by passing them as arguments to the save() function. • It is more flexible than usethis::use_data() because it allows you to save any object, not just datasets. 4.3.7 Saving Data Using saveRDS() for .RDS Files Another method for saving datasets is by using the saveRDS() function, which saves a single R object to a .RDS file. Unlike .rda files, .RDS files are designed for saving a single object. The readRDS() function is used to load these objects back into R. saveRDS(fmc2017, file = &quot;data/fmc2017.rds&quot;) This will save the fmc2017 dataset as an .RDS file in the data/ directory. You can later load it using readRDS(). 4.3.7.1 Features of .RDS files: • .RDS files store a single R object, making them ideal for storing individual datasets. • They are useful when you want more control over how the object is loaded back into R, as you need to assign the object to a variable after loading it. 4.3.8 When to Use .rda vs. .RDS • Use .rda if you want to save multiple objects together in one file (e.g., different datasets or objects used by your package). This is the preferred method when using usethis::use_data(). • Use .RDS when you want to save a single object, especially if you need to load it into R at a later time and assign it to a variable immediately. 4.3.9 Using Other File Formats While .rda and .RDS are the primary formats used for saving R objects within packages, R can also work with other formats depending on your needs: • CSV Files: If you prefer to save your dataset in a more universal format, you can save your dataset as a CSV file using write.csv(). While this is not an R-specific format, it is widely used for sharing data. write.csv(fmc2017, file = &quot;data/fmc2017.csv&quot;, row.names = FALSE) • Excel Files: If your dataset is more complex and needs to be shared as an Excel file, you can use the write.xlsx() function from the openxlsx package to save your data. openxlsx::write.xlsx(fmc2017, &quot;data/fmc2017.xlsx&quot;) These formats can be useful for exporting data outside of R, but for R package development, .rda and .RDS remain the most efficient and flexible formats. 4.4 Choosing the Right Location for Your Data When developing an R package or project, selecting the right location for storing data is critical for both functionality and performance. This section discusses the various storage locations within an R package, the purpose of each, and the best practices for organizing your datasets. 4.4.1 Storing Data in data/ (Public Datasets for Users) The data/ directory is the primary location for storing processed data that will be included in the R package for public use. This is the most common place for small-to-medium datasets that you want to make available to the users of your package. • Purpose: The datasets stored here are loaded lazily by default, which means they are not loaded into memory until they are explicitly needed. This reduces memory consumption and allows for faster loading of the package itself. • How to Save Data: To save an object to this directory, use the usethis::use_data() function. For example: usethis::use_data(my_dataset, overwrite = TRUE) This command will save the dataset as data/my_dataset.rda. • How to Access Data: Once the package is installed, users can load the data using the data() function: data(&quot;my_dataset&quot;) To inspect the dataset, users can use: head(my_dataset) • Why This Location Exists: • Lazy loading: Datasets are only loaded into memory when needed. • User-friendly: The data() function makes it easy for users to access datasets. • Efficient for small-to-medium datasets: It works well for datasets that are not large, as these datasets are included in the package itself. • Who Has Access: • Users: Can access the dataset by loading it with the data() function. • Package Developers: Can modify and update the dataset before release. 4.4.2 Storing Data in data-raw/ (Raw Data for Processing Only) The data-raw/ directory is used for storing raw data files before they are cleaned and processed. This directory is meant for developers to preprocess data before it is stored in the data/ directory. • Purpose: The data-raw/ directory should contain scripts for data cleaning and transformation, as well as raw data files like .csv, .json, or .xlsx. • How to Save Data: Once the raw data has been processed, use the usethis::use_data() function to save the cleaned dataset to data/: usethis::use_data(cleaned_fmc2017, overwrite = TRUE) • How to Access Raw Data: The raw data is not bundled in the final package, so developers need to manually read the data files for preprocessing: read.csv(&quot;data-raw/raw_data.csv&quot;) • Why This Location Exists: • For reproducibility: Storing the raw data and the preprocessing code ensures that the data can be recreated in the future. • For preprocessing: Raw data needs to be cleaned or transformed before it is included in the package. • Not included in the installed package: This keeps the package size smaller and prevents unnecessary data from being distributed to users. • Who Has Access: • Users: Do not have access to the raw data because it is not included in the installed package. • Package Developers: Have full access to preprocess the raw data. 4.4.3 Storing Data in inst/extdata/ (External Files) The inst/extdata/ directory is used for external data files that are part of your package but not lazily loaded. This directory is suitable for larger datasets or files that users may want to modify. • Purpose: The inst/extdata/ directory is good for storing large datasets, or files that need to be manually accessed by users. • How to Save Data: Store external data files such as .csv, .json, .xlsx, etc., inside the inst/extdata/ directory. • How to Access Data: Users can read files from inst/extdata/ using system.file() to get the full path: file_path &lt;- system.file(&quot;extdata&quot;, &quot;my_data.csv&quot;, package = &quot;myPackage&quot;) read.csv(file_path) • Why This Location Exists: • Good for large datasets: Since the data is not lazily loaded, it is suitable for large datasets that users can access directly. • Allows updates: Users can modify the external data without rebuilding the entire package. • Useful for raw files: This is a good place to store files that are not processed within the package. • Who Has Access: • Users: Can manually access the external data files. • Package Developers: Can use the external files within functions in the package. 4.4.4 Storing Data in R/ (Data as R Scripts) The R/ directory is intended for storing data as R code, defining datasets directly within R scripts. This method is suitable for small, simple datasets that do not require complex preprocessing or storage. • Purpose: The R/ directory allows you to generate datasets dynamically using R code. It’s great for small datasets or when datasets are generated by functions rather than being loaded from external files. • How to Save Data: Define the dataset in an R script inside the R/ directory: my_dataset &lt;- data.frame( id = 1:5, name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;, &quot;David&quot;, &quot;Emma&quot;), salary = c(5000, 6000, 7000, 7200, 5500) ) • How to Access Data: The dataset is available once the package is loaded: my_dataset • Why This Location Exists: • Dynamic data generation: Useful for datasets that need to be generated on the fly or based on functions. • Compact and readable: Ideal for small or simple datasets. • Useful for function-generated data: If the dataset depends on a function call, this is a convenient place to store it. • Who Has Access: • Users: Can access the dataset once the package is loaded. • Package Developers: Can modify and update the dataset. 4.4.5 Storing Data in R/sysdata.rda (Internal Data for Package Functions) The R/sysdata.rda file is used to store internal datasets that are not meant to be accessed by users directly. These datasets are only available within package functions. • Purpose: The sysdata.rda file is used for precomputed data, lookup tables, or internal information needed by functions within the package. • How to Save Data: Store the dataset using usethis::use_data() with the internal = TRUE argument: usethis::use_data(my_internal_dataset, internal = TRUE, overwrite = TRUE) • How to Access Data: The dataset is available only within package functions: my_function &lt;- function() { return(my_internal_dataset) } • Why This Location Exists: • For internal use only: Keeps sensitive or proprietary data hidden from users. • Prevents clutter: Keeps the user environment clean by not exposing internal datasets. • Optimized for performance: Data stored here is ready for fast lookups within package functions. • Who Has Access: • Users: Cannot access the internal data directly. • Package Developers: Can use it inside functions within the package. 4.5 Summary Choosing the right location for your data is essential for the functionality, size, and accessibility of your R package. By understanding the different directories available and their purposes, you can better organize your data for your project: • data/: Store processed, public datasets that should be lazily loaded for users. • data-raw/: Store raw, unprocessed data and scripts for cleaning and transformation. • inst/extdata/: Store large or user-modifiable external data files. • R/: Store small, simple datasets that are generated via R code. • R/sysdata.rda/: Store internal, precomputed datasets used by functions. Each storage method has its place depending on the nature of the data and the needs of your package users. By organizing your data correctly, you ensure efficient memory usage and provide clear separation between raw data, processed data, and internal functions. 4.6 Checking and Loading Stored Data Ensuring that your data is correctly stored and accessible within your R package is a crucial part of the development process. This section will guide you through the steps to verify that the data has been stored correctly in your package and how to load it for use or troubleshooting. 4.6.1 Check if the Data is Properly Stored When developing an R package, the data that you store within the data/ directory should be accessible and correctly formatted. You can verify this by running a few simple commands to ensure the data exists in the expected location. Checking for Stored Data: You can use the data() function to list all available datasets in your package. For instance, if your package is called nigeriaPITaudit, you can use: data(package = &quot;nigeriaPITaudit&quot;) This command will list all datasets bundled with the package, including their names, descriptions, and locations. If fmc2017.rda is correctly stored in the data/ folder, it should appear in the list. Alternatively, you can use the list.files() function to confirm the physical presence of your dataset file in the correct directory. Specifically, this will check the data/ folder inside your package’s structure: list.files(&quot;data&quot;) If the dataset fmc2017.rda is present, it should be listed. If it is missing, you may need to troubleshoot why the dataset wasn’t properly saved. 4.6.2 Re-run in a Clean Environment When developing a package, it is important to ensure that the data is properly saved and loaded across environments. This is particularly important when you want to ensure that everything functions correctly in a clean, fresh setup. To simulate this, you can create a clean environment by using the devtools::check() function, which checks your package’s correctness and ensures that the data files are stored properly. The clean = TRUE argument ensures that you’re testing the package in a fresh environment, free from any previous session’s variables. devtools::check(clean = TRUE) Running this command checks your package’s structure, the documentation, the data files, and more, to ensure that everything works correctly in a clean environment. If everything is set up properly, fmc2017.rda should be accessible as part of your package’s dataset when you load the package in a new R session. 4.6.3 Accessing the Data After Check After verifying that the data has been stored and the package works correctly in a clean environment, you can proceed to load the data into your R session. The easiest way to load data from the data/ directory is by using the data() function: data(&quot;fmc2017&quot;) This command loads the dataset into your current environment. Once the data is loaded, you can inspect it using standard R functions like head(), summary(), or str() to ensure it is in the correct format: head(fmc2017) 4.6.4 Troubleshooting Data Loading Issues If you are unable to load the dataset as expected, here are a few common troubleshooting steps to consider: • Check File Path: Ensure that the dataset is located in the data/ folder within your package’s directory. The list.files(“data”) command should confirm the presence of your dataset file (fmc2017.rda). • Re-run use_data(): If the dataset isn’t appearing, make sure you’ve saved the data properly using the usethis::use_data() function. This function should be used to save your dataset into the package: usethis::use_data(fmc2017, overwrite = TRUE) • Check for Errors: During development, errors may occur if the data file is corrupted, or if you attempted to load data without calling use_data() to properly store it. Re-run the devtools::check() function to ensure that the package structure is intact. 4.6.5 Conclusion Why Checking and Loading Data is Important: Regularly checking that your data is properly stored ensures the reproducibility of your R package. It also allows users who install your package to access the data seamlessly without issues. Verify Clean Package Installation: Using the devtools::check(clean = TRUE) function ensures that your package works as expected in a clean environment. This is essential for confirming that your data and package setup are correct, especially if you are preparing to share your package with others. By following these steps and ensuring that your data is stored and accessed properly, you can make sure that your package remains robust and user-friendly, providing a smooth experience for users of your package. "],["documenting-datasets.html", "Chapter 6 Documenting Datasets", " Chapter 6 Documenting Datasets This chapter focuses on the importance of documenting datasets in R packages. Clear documentation ensures users can understand and effectively use the data. We cover best practices for writing dataset documentation, using roxygen2, and integrating documentation into the package development workflow for consistency and usability. 5.1 Why Documenting Datasets Matters In the context of R packages, clear and thorough documentation is crucial for making your datasets accessible, usable, and reproducible. Documentation serves as a guide for other users and developers to understand the structure, source, and content of the data you’re sharing. It provides transparency and builds trust, helping users know exactly what they’re working with, how to use the data, and how it fits into the broader goals of your R package. 5.1.1 Importance of Clear Documentation Documenting your datasets clearly allows other users to: • Understand the structure and contents: What variables are included? What do the variables represent? What are the units of measurement? Without clear documentation, users may misinterpret the data or fail to use it properly. • Understand data sources: Knowing where the data came from and how it was collected is crucial for assessing its reliability and relevance. This is especially important for datasets used in scientific research or policy-making. • Ensure reproducibility: Reproducible research is a core principle of modern science. By documenting how the dataset was created, cleaned, and processed, others can replicate your results. • Maximize reusability: Users can apply datasets to new problems or use them as examples in their own work when they know exactly what the data represents and how it can be manipulated. 5.1.2 Benefits for Users and Developers For Users: • Improved understanding: With good documentation, users can easily grasp the dataset’s structure and content without having to spend time figuring it out themselves. • Ease of use: Users can apply the dataset in their own projects without confusion, reducing the learning curve associated with unfamiliar datasets. • Effective error detection: Clear documentation can help users identify when their analyses might be incorrect or assumptions may not align with the data, minimizing the risk of errors. For Developers: • Clearer codebase: When you document datasets properly, you not only help users but also make your own codebase easier to maintain and improve. Others who contribute to the package will find it easier to understand your work. • Better package adoption: Well-documented datasets are more likely to be adopted by users, which in turn increases the overall impact of your package. • Compliance with standards: Well-documented data helps meet open-source and academic standards, especially in contexts where datasets must be transparent and usable by others (e.g., public datasets or research-grade data). 5.1.3 Best Practices Overview To ensure that your datasets are well-documented, you can follow these best practices: Use roxygen2 tags: Use the @docType, @format, @source, and @examples tags to provide clear and consistent documentation for each dataset in your R package. Describe the structure: Include detailed information about the columns, their types (e.g., numeric, character), and any units of measurement. Specify the number of rows and any known issues (e.g., missing values). Document the source and citation: Always mention where the dataset came from, who created it, and provide citation information if the dataset is publicly available or needs attribution. Provide example usage: Give examples of how to load and use the dataset, along with common operations (e.g., subsetting, summary statistics). Explain preprocessing steps: If you’ve transformed the data in any way (e.g., cleaning, aggregating), be sure to document these steps in the dataset’s description. This helps users understand how the data was processed and avoid any confusion. 5.2 Overview of the Documentation Workflow Documenting datasets is an essential part of developing an R package. It ensures that users can understand and effectively use the data. The documentation workflow can be broken down into a high-level process: High-level Flow: Write → Test → Build → Check Write: Begin by writing the documentation for your dataset in a file within the R/ directory. This is done using roxygen2 comments, which provide metadata and explanations for the dataset. Test: After writing the documentation, test whether it is correctly formatted and functional. Check for errors in the documentation and ensure that the dataset loads and functions properly. Build: Once documentation and code are written, build the package. This includes compiling all the scripts and datasets into a complete package that can be shared or used. Check: Run checks to verify the integrity and completeness of the documentation. This step ensures that all documentation is compliant with R package standards and that there are no errors. Where Documentation Fits in Package Development Documentation is an integral part of the package development process, as it serves as the bridge between the code and the users. It should be done early in the package development process and continuously updated as changes are made. By documenting your datasets well, you are ensuring that users can easily understand and use your data without needing to dive into the raw code. As you develop functions or update datasets, keep documentation in sync to ensure consistency throughout the package. 5.3 Creating a Dataset Documentation File The documentation for datasets in an R package is typically stored in .R files within the R/ directory. This is where you define the metadata about the dataset and provide information that users will need to understand it. Where the Documentation Lives • R/*.R Files: Dataset documentation is stored as comments in .R files located inthe R/ folder of the package. These files are often associated with the dataset and are typically named after the dataset they document (e.g., my_dataset.R for my_dataset). • Naming Conventions: Follow consistent naming conventions to ensure that each dataset has a corresponding documentation file. This should match the dataset’s name and provide easy access for users and developers. How Dataset Docs Differ from Function Docs Dataset documentation has some key differences compared to function documentation: • Dataset documentation: Focuses on providing a description of the data structure (e.g., columns, types), the source of the data, and how it can be used. It may include information on preprocessing steps or transformations applied to the data. • Function documentation: Describes the function’s purpose, arguments, return values, and examples of how to use it. While function documentation is focused on how to execute a task, dataset documentation is focused on describing the data that is being used within those tasks. 5.4 Writing with roxygen2 The roxygen2 package is the primary tool used for documenting datasets in R. It allows you to write documentation directly in the code file, using special tags that are processed to generate .Rd files (which contain the help documentation for R objects). Using roxygen2 for Dataset Docs: With roxygen2, you can use special comment tags to document datasets. The documentation is written directly above the dataset’s definition in .R files. Minimal vs Detailed Documentation • Minimal Documentation: The minimum documentation for a dataset includes a brief description of the dataset, its source, and how to use it. • Detailed Documentation: More comprehensive documentation includes the dataset’s structure, units of measurement, details of preprocessing or cleaning steps, and any transformations performed on the data. Tag Examples Here are some commonly used roxygen2 tags for dataset documentation: • @docType dataset: Specifies that this is a dataset (not a function). • @format: Describes the data structure (e.g., number of rows, columns, types). • @source: Provides information about where the data comes from or how it was collected. • @examples: Provides examples of how to load and use the dataset in R. Example: #&#39; @docType dataset #&#39; @format A data frame with 100 rows and 3 variables: #&#39; \\describe{ #&#39; \\item{age}{Age of the employee (numeric)} #&#39; \\item{salary}{Salary of the employee (numeric)} #&#39; \\item{department}{Department the employee belongs to (factor)} #&#39; } #&#39; @source Internal payroll database #&#39; @examples #&#39; data(employee_data) #&#39; head(employee_data) &quot;employee_data&quot; 5.5 Key Components of Dataset Documentation Dataset documentation must contain several key components to ensure that users can fully understand and properly use the data. Here are the key sections to include: 5.5.1 Title – One-line Summary The title is a concise, one-line summary that gives the user an immediate understanding of the dataset. It should be descriptive enough to convey the purpose of the data. Example: #&#39; Employee Data for 2021 5.5.2 Description – What the Data Is, Where It Comes From The description provides context for the dataset. It should include what the dataset represents, who collected it, and why it was collected. This section can also explain any transformations or preprocessing steps that were applied to the data. Example: #&#39; A dataset containing employee information for the year 2021. #&#39; Includes salary details, department, and employee age. #&#39; The data was collected from the company’s payroll database. 5.5.3 Format – Data Structure (Rows, Columns, Types) The format section describes the dataset’s structure, including the number of rows, the column names, and the types of data in each column (e.g., numeric, factor). Example: #&#39; @format A data frame with 100 rows and 3 variables: #&#39; \\describe{ #&#39; \\item{age}{Employee’s age (numeric)} #&#39; \\item{salary}{Employee’s annual salary (numeric)} #&#39; \\item{department}{Department (factor)} #&#39; } 5.5.4 Source – Reference to External Origin or Publication The source section provides details about where the data came from. This could be a publication, a website, or an internal database. This helps ensure that users can verify and trust the data. Example: #&#39; @source Company payroll database, retrieved in January 2021. 5.5.5 Examples – How to Load/Use the Dataset The examples section shows how users can load and use the dataset in their R code. This can include code snippets demonstrating common operations on the dataset. Example: #&#39; @examples #&#39; data(employee_data) #&#39; summary(employee_data) #&#39; head(employee_data) 5.6 Building and Checking Documentation Once your dataset documentation is complete, it’s important to build and check it to ensure that everything is working properly. devtools::document() The devtools::document() function processes all roxygen2 comments in your code files and generates the corresponding .Rd files in the man/ directory. This is an important step in creating the package documentation. Example: devtools::document() Viewing with ?datasetName: After building the documentation, you can view it by typing ?datasetName in the R console. This will bring up the documentation for the dataset, allowing users to easily access the information. Example: ?employee_data devtools::check() to Validate Documentation Use devtools::check() to run a comprehensive check of your R package. This will validate the correctness of your documentation, ensuring that all required fields are filled and that there are no errors. Example: devtools::check() "],["best-practices-for-including-datasets-in-r-packages.html", "Chapter 7 Best Practices for Including Datasets in R Packages", " Chapter 7 Best Practices for Including Datasets in R Packages Including datasets in an R package enhances usability, supports examples, and offers reproducibility. However, it’s important to follow best practices to ensure your datasets are efficient, easy to maintain, and accessible to users. This chapter covers practical guidelines for managing datasets effectively. 6.1 Keeping Datasets Small and Efficient One of the most important practices when including datasets in your R package is minimizing size without sacrificing quality. Large datasets can significantly slow down your package, increase installation time, and make it harder for users to explore or load data interactively. 6.1.1 Why Keep Datasets Small? • Smaller datasets load faster and consume less memory. • CRAN recommends packages not exceed 5MB. • Users can explore smaller datasets more easily. • It improves portability and package performance. 6.1.2 Techniques for Reducing Dataset Size • Include only relevant columns Avoid storing unnecessary fields. If your analysis only needs 5 columns, don’t include all 50 from the raw data. minimal_data &lt;- original_data[, c(&quot;employee_id&quot;,&quot;gross_salary&quot;, &quot;tax_paid&quot;)] • Use efficient data types Convert character strings to factors where appropriate. Avoid storing large objects in inefficient formats. dataset$employer_type &lt;- as.factor(dataset$employer_type) • Sample the data Instead of uploading the full dataset, especially if it’s huge, take a representative sample: set.seed(123) sampled_data &lt;- dplyr::sample_n(full_data, 500) • Compress your data Use the compress argument in usethis::use_data() for storage: usethis::use_data(my_data, compress = &quot;xz&quot;) Compression types include: • “xz” – best compression • “bzip2” – medium compression • “gzip” – fast, but larger size • Eliminate redundancy Do not include the same dataset in multiple formats (e.g., both CSV and .rda). Stick to one, ideally .rda. • Summarize instead of storing full data If your goal is only to share results or statistics, store summaries instead of raw data: summary_stats &lt;- data.frame( avg_salary = mean(data$gross_salary), median_tax = median(data$tax_paid) ) 6.1.3 Quick Checklist Before you include a dataset, ask yourself: • Does this dataset exceed 5MB? • Can I reduce the number of rows or columns? • Is it compressed? • Are the variable types optimized? • Is it truly necessary to include the full data? 6.2 Providing Clear and Complete Documentation Thorough documentation is critical when including datasets in R packages. It helps users understand the dataset’s structure, origin, and intended use — and ensures your package is intuitive and accessible. 6.2.1 Use roxygen2 Tags The easiest and most consistent way to document datasets in R packages is with the roxygen2 package. Dataset documentation goes in a script placed in the R/ directory (not in data/). For example, if your dataset is staff_payroll, you might create a file R/staff_payroll.R. Use the following roxygen2 tags: • @format: Describe the structure of the dataset (number of rows, columns, types). • @source: Mention where the data came from (URL, paper, manual entry, etc.). • @examples: Provide one or more examples to help users load or use the dataset. #&#39; Payroll dataset for a typical Nigerian firm #&#39; #&#39; A dataset containing employee payroll data used in PAYE auditing. #&#39; #&#39; @format A data frame with 200 rows and 6 variables: #&#39; \\describe{ #&#39; \\item{name}{Employee full name} #&#39; \\item{gross_salary}{Monthly gross salary in NGN} #&#39; \\item{pension}{Pension deduction} #&#39; \\item{nhf}{National Housing Fund deduction} #&#39; \\item{tax_paid}{Total tax remitted} #&#39; \\item{month}{Payroll month} #&#39; } #&#39; @source Internal computation from sample audit files #&#39; @examples #&#39; data(staff_payroll) #&#39; head(staff_payroll) &quot;staff_payroll&quot; 6.2.2 Match Dataset Names with .Rd Files The documentation files generated from roxygen2 will live in the man/ folder as .Rd files, named after your dataset. If your dataset is tax_data_2023, then the file in man/ will be tax_data_2023.Rd. This helps R find and display help files correctly when users run ?tax_data_2023. 6.2.3 Include Usage Examples Usage examples are essential for helping users explore the data quickly. Provide at least one working example that uses basic R functions like head(), summary(), or even custom functions from your package: @examples: data(tax_data_2023) summary(tax_data_2023) plot(tax_data_2023$gross_salary, tax_data_2023$tax_paid). Avoid complex or long examples in the documentation — keep them short and focused. 6.3 Using Consistent and Descriptive Naming Conventions Naming your datasets clearly and consistently improves usability, supports better code readability, and prevents confusion — especially as your package grows in size and scope. 6.3.1 Choose a Consistent Naming Style Two common naming styles in R are: • snake_case — e.g., employee_salaries, tax_summary_2022 • lowerCamelCase — e.g., employeeSalaries, taxSummary2022 Pick one style and use it across all datasets and functions in your package. snake_case is more common in datasets. 6.3.2 Avoid Generic or Confusing Names Bad names make it harder for users to remember what a dataset is for. Avoid vague names like data1, test, or my_data. Use descriptive names that convey the content and purpose of the dataset. Examples of good names: • lagos_payroll_2022 • audited_tax_remittance • federal_employees_nov2023 6.3.3 Match Dataset Names with R Files in data-raw/ If you preprocess or generate data in scripts stored in data-raw/, make sure the file name reflects the dataset it creates. For example: • data-raw/prepare_lagos_payroll.R should generate lagos_payroll.rda • data-raw/build_tax_summary_2022.R should save tax_summary_2022.rda This practice improves traceability and maintenance of your package. Perfect! Let’s continue with the next sections of Chapter 6: Best Practices for Including Datasets in R Packages, keeping the structure clean, readable, and aligned with best practices. 6.4 Keeping Raw Data Scripts in data-raw/ Folder Managing how datasets are generated is just as important as including them in your R package. The data-raw/ folder is the conventional place to store raw data scripts — helping ensure reproducibility and transparency. 6.4.1 Use usethis::use_data_raw() The usethis package provides a helpful function, use_data_raw(“dataset_name”), which: • Creates a new R script in the data-raw/ directory • Sets up boilerplate code for loading, cleaning, and saving data • Encourages clean separation between raw and processed data usethis::use_data_raw(&quot;staff_payroll&quot;) 6.4.2 Ensure Reproducibility Always write scripts in data-raw/ that can regenerate your .rda datasets from scratch. This includes loading raw files (CSV, Excel), cleaning data, and saving it using usethis::use_data(): raw_data &lt;- read.csv(&quot;data-raw/staff_raw.csv&quot;) clean_data &lt;- clean_staff_data(raw_data) usethis::use_data(clean_data, overwrite = TRUE) This process helps users and contributors understand how your data was created and allows others to reproduce it reliably. 6.5 Testing Your Datasets Testing isn’t just for functions — your datasets should be tested too. This ensures consistency and prevents bugs when your package grows or changes. 6.5.1 Use testthat to Validate Datasets The testthat package allows you to write tests that check for: • Correct column names and data types • Expected number of rows or columns • Missing values or invalid entries Place these tests in the tests/testthat/ folder: test_that(&quot;staff_payroll has correct columns&quot;, { expect_named(staff_payroll, c(&quot;name&quot;, &quot;gross_salary&quot;, &quot;pension&quot;, &quot;nhf&quot;, &quot;tax_paid&quot;, &quot;month&quot;)) }) test_that(&quot;no missing values in tax_paid&quot;, { expect_false(any(is.na(staff_payroll$tax_paid))) }) 6.5.2 Automate with devtools::test() You can run all dataset (and package) tests automatically using: devtools::test() This encourages a data-aware development cycle and early bug detection. 6.6 Avoiding Proprietary or Sensitive Data It’s crucial to be mindful of what data you’re including in your package — especially when sharing it publicly. 6.6.1 Don’t Include Data with License Issues Ensure your data source is public, licensed for redistribution, or created by you. If you’re unsure about the license, do not include it in your package. Avoid including: • Third-party data without permission • Files extracted from proprietary software (like Excel templates from clients) • Confidential payroll or tax documents 6.6.2 Anonymize Sensitive Information If your dataset contains real-world people or companies, take care to: • Remove names, IDs, or phone numbers • Replace company names with placeholders like “Company A”, “Ministry B” • Round or fuzz salary figures to prevent re-identification 6.7 Including Data Licenses and Citations Just like software, datasets should include licenses and citations when appropriate. 6.7.1 Use the @source Field for Citations In your roxygen2 documentation, the @source tag should include a citation or URL to the original dataset: #’ @source Lagos State Tax Office payroll audit, 2023 For publicly available datasets, always include a clickable source or citation information. 6.7.2 Mention Licenses in DESCRIPTION If your dataset has a specific license (e.g., CC-BY 4.0), mention it in the DESCRIPTION file under a dedicated DataLicense: or include it in the License: field. You can also describe the license in the dataset documentation file. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
